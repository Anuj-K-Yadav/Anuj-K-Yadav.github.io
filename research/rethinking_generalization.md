---
title: 'Understanding Deep Learning Requires Rethinking Generalization'
date: 2020-06-12
permalink: /posts/rps.md/
tags:
  - Machine Learning
  - Research Papers
  - Summaries
  - Resources
---

###  [Understanding Deep Learning Requires Rethinking Generalization](https://bengio.abracadoudou.com/cv/publications/pdf/zhang_2017_iclr.pdf) **ICLR 2017**

<p>In the following paper, the authors question the traditional way to think of small generalization error exhibited by Neural networks as a consequence of the class of the Network architecture or the Regularization technique used during training.  Authors show that despite having a well-proven architecture or having a Regularization technique, these architectures can fail to explain the low generalization error.
The authors do this by a series of intuitive experiments showing that a Neural network can learn/memorize all the data in the training set. Hence, giving low error on training data and large error and test data, which means that the model has overfitted the data. The authors show that this observation is unaffected even after using certain regularization techniques.
Authors start by training the model on randomly labeled data and gradually increase the number of corrupted labels, it is observed that the training accuracy in all cases is close to 100% The observation seems to be shocking but if thought about clearly, this seems possible if the Network is provided with sufficient parameters to memorize all the training points (which is the case in AlexNet which has about 1.4M parameters and InceptionNet which has about 1.7M parameters). The authors do the same experiment with Random & Shuffled image pixels, yet the models give very high training accuracy. This supports the argument that the Model can learn all the data because the data, in this case, is NOT "learnable", so the high accuracy must imply that all the data is Memorized. What is surprising though is that, the rate of convergence in the case of Random & Shuffled pixels is faster than that of Random labels, in fact, time for convergence on Shuffled pixels is almost the same as that on True labels. 
The authors then comment on the role of Regularization. The authors take 3 commonly used explicit regularization techniques- Data Augmentation, Weight Decay & Dropouts. The observation in this case with original labels is that the Regularizers indeed help the models to generalize better on the Test data but the improvement is not very significant (Table 2 in the paper). Whereas with Random labels, it can be seen that the Training accuracy has deteriorated but still it's close to 90% which is competent, the Test accuracy in these cases is well below 1%. On the other side when Implicit regularization methods like Early Stopping & Batch Normalization are used, the Test accuracy on Original labels does improve, but not very significantly (only about 3~6%). 
These sets of experiments show that Regularizers indeed improve the generalization marginally but they don't seem to be the fundamental reason for generalization capability of Neural networks.
After this, the authors analyze the finite-sample expressivity of Neural nets and provide a Heuristic proof for existence of a 2-layer neural network with sufficient parameters which can represent any function (condition to number of parameters).
At last, the author argue that even in case of simple models like Linear Models, it is difficult to understand the source of generalization. Authors propose that with enough number of Training samples, there can be multiple Global minima possible for Emperical Risk Minimization, but all of these global minimum may not generalize well! Neither can we distinguish them on the basis of Curvature of loss function (as the curvature at all minima will be same for Linear model). Here, the authors provide an interesting direction to consider the solution achieved by SGD which would be unique!
Further this method provides a Test error of 1.2% on MNIST which is quite surprising (considering such a simple model) but the memory consumption is impractical (30GB) with further pre-processing of wavelet transform, the Test error further reduces to 0.6%. Similar result on CIFAR show improved test errors. But, this maynot be considered as the key for generalization error as the point of convergence of SGD is the one with minimum L2 norm, but with wavelet transform applied the norm increases but still the test error drops.
These findings are really interesting and changes one's perspective to look at Neural nets as something which learns to generalize & distinguish classes. It also opens up new directions to look upon!</p>